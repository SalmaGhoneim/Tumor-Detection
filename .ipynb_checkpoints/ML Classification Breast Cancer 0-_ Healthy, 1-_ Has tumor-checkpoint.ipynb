{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random \n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw image\n",
    "\n",
    "def viewImage(image):\n",
    "    cv2.namedWindow('Display', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('Display', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_info = open('tumor_info.txt', 'r')\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "\n",
    "test_pos = []\n",
    "test_neg = []\n",
    "\n",
    "if(tumor_info.mode == 'r'):\n",
    "    info = tumor_info.read()\n",
    "\n",
    "elements = info.split('\\n')\n",
    "for element in elements:\n",
    "    element_split = element.split(' ')\n",
    "    if(len(element_split) > 1 ):\n",
    "        element_split_split = element_split[0].split('b')\n",
    "\n",
    "        if(element_split[2] == 'NORM'):\n",
    "            if(int(element_split_split[1]) < 258):\n",
    "                train_neg.append(element_split_split[1])\n",
    "            else: \n",
    "                test_neg.append(element_split_split[1])\n",
    "\n",
    "        else:\n",
    "            if(int(element_split_split[1]) < 258):\n",
    "                train_pos.append(element_split_split[1])\n",
    "            else:\n",
    "                test_pos.append(element_split_split[1])\n",
    "                \n",
    "\n",
    "train_pos = list(set(train_pos))\n",
    "train_pos.sort()\n",
    "\n",
    "train_neg = list(set(train_neg))\n",
    "train_neg.sort()\n",
    "\n",
    "test_pos = list(set(test_pos))\n",
    "test_pos.sort()\n",
    "\n",
    "test_neg = list(set(test_neg))\n",
    "test_neg.sort()\n",
    "\n",
    "\n",
    "# print(train_neg)\n",
    "# print(len(train_neg))\n",
    "# print(train_pos)\n",
    "# print(len(train_pos))\n",
    "\n",
    "# print(test_neg)\n",
    "# print(len(test_neg))\n",
    "# print(test_pos)\n",
    "# print(len(test_pos))\n",
    "\n",
    "\n",
    "train_images = train_pos + train_neg\n",
    "random.shuffle(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/build/opencv/src/opencv-3.4.0/modules/imgproc/src/color.cpp:11079: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c28b52dc555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_for_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnew_image\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimgplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /build/opencv/src/opencv-3.4.0/modules/imgproc/src/color.cpp:11079: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg \n",
    "path_for_images = 'new_train_images/'\n",
    "for img in train_images[0:3]:\n",
    "    print(img)\n",
    "    image = mpimg.imread(path_for_images + img + '.jpg', 0)\n",
    "    new_image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    imgplot = plt.imshow(new_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_pixels = []\n",
    "train_labels = []\n",
    "nrows = 150\n",
    "ncols = 150\n",
    "\n",
    "for image in train_images:\n",
    "    train_images_pixels.append(cv2.resize(cv2.imread(path_for_images + image + '.jpg', cv2.IMREAD_COLOR), \n",
    "                               (nrows, ncols), interpolation= cv2.INTER_CUBIC))\n",
    "    \n",
    "    if(image in train_pos):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)  \n",
    "# print(train_images_pixels)\n",
    "# print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Labels for pos & neg')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICRJTf8fkfI9qL2suDsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x696b837e5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_images_pixels = np.array(train_images_pixels)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "sns.countplot(train_labels)\n",
    "plt.title('Labels for pos & neg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train image is:  (257, 150, 150, 3)\n",
      "Shape of labels is:  (257,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train image is: ', train_images_pixels.shape)\n",
    "print('Shape of labels is: ', train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train images is  (205, 150, 150, 3)\n",
      "Shape of validation images is  (52, 150, 150, 3)\n",
      "Shape of labels is  (205,)\n",
      "Shape of validation labels is  (52,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images_pixels, train_labels, test_size = 0.2, random_state=2)\n",
    "\n",
    "print('Shape of train images is ', X_train.shape)\n",
    "print('Shape of validation images is ', X_val.shape)\n",
    "print('Shape of labels is ', y_train.shape)\n",
    "print('Shape of validation labels is ', y_val.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = len(X_train)\n",
    "nval = len(X_val)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))  #Dropout for regularization\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer= optimizers.RMSprop(lr= 1e-7), metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.6907 - acc: 0.5680 - val_loss: 0.6905 - val_acc: 0.6250\n",
      "Epoch 2/128\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.6877 - acc: 0.5754 - val_loss: 0.6903 - val_acc: 0.5833\n",
      "Epoch 3/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6869 - acc: 0.5814 - val_loss: 0.6941 - val_acc: 0.5833\n",
      "Epoch 4/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6906 - acc: 0.5646 - val_loss: 0.6879 - val_acc: 0.6667\n",
      "Epoch 5/128\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.6885 - acc: 0.5877 - val_loss: 0.6902 - val_acc: 0.6250\n",
      "Epoch 6/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6865 - acc: 0.6146 - val_loss: 0.6929 - val_acc: 0.5278\n",
      "Epoch 7/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6893 - acc: 0.5416 - val_loss: 0.6882 - val_acc: 0.7222\n",
      "Epoch 8/128\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.6883 - acc: 0.5794 - val_loss: 0.6911 - val_acc: 0.5833\n",
      "Epoch 9/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6873 - acc: 0.6094 - val_loss: 0.6897 - val_acc: 0.6458\n",
      "Epoch 10/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6888 - acc: 0.5676 - val_loss: 0.6904 - val_acc: 0.5833\n",
      "Epoch 11/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6887 - acc: 0.5847 - val_loss: 0.6928 - val_acc: 0.5833\n",
      "Epoch 12/128\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.6905 - acc: 0.5833 - val_loss: 0.6889 - val_acc: 0.6389\n",
      "Epoch 13/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6893 - acc: 0.5687 - val_loss: 0.6909 - val_acc: 0.6042\n",
      "Epoch 14/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6881 - acc: 0.5866 - val_loss: 0.6912 - val_acc: 0.6111\n",
      "Epoch 15/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6857 - acc: 0.5970 - val_loss: 0.6847 - val_acc: 0.7222\n",
      "Epoch 16/128\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.6877 - acc: 0.6042 - val_loss: 0.6942 - val_acc: 0.5278\n",
      "Epoch 17/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6918 - acc: 0.5479 - val_loss: 0.6916 - val_acc: 0.5833\n",
      "Epoch 18/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6924 - acc: 0.5462 - val_loss: 0.6907 - val_acc: 0.6111\n",
      "Epoch 19/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6866 - acc: 0.5990 - val_loss: 0.6874 - val_acc: 0.6667\n",
      "Epoch 20/128\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6908 - acc: 0.5397 - val_loss: 0.6904 - val_acc: 0.6111\n",
      "Epoch 21/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6857 - acc: 0.6328 - val_loss: 0.6904 - val_acc: 0.6042\n",
      "Epoch 22/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6902 - acc: 0.5574 - val_loss: 0.6899 - val_acc: 0.6111\n",
      "Epoch 23/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6900 - acc: 0.5885 - val_loss: 0.6913 - val_acc: 0.6111\n",
      "Epoch 24/128\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6892 - acc: 0.5564 - val_loss: 0.6882 - val_acc: 0.6389\n",
      "Epoch 25/128\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6878 - acc: 0.6212 - val_loss: 0.6904 - val_acc: 0.6042\n",
      "Epoch 26/128\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.6912 - acc: 0.5710 - val_loss: 0.6847 - val_acc: 0.7222\n",
      "Epoch 27/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6876 - acc: 0.5866 - val_loss: 0.6962 - val_acc: 0.4722\n",
      "Epoch 28/128\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.6878 - acc: 0.5781 - val_loss: 0.6880 - val_acc: 0.6667\n",
      "Epoch 29/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6891 - acc: 0.5646 - val_loss: 0.6899 - val_acc: 0.6042\n",
      "Epoch 30/128\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6884 - acc: 0.5720 - val_loss: 0.6932 - val_acc: 0.5556\n",
      "Epoch 31/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6900 - acc: 0.5677 - val_loss: 0.6866 - val_acc: 0.6944\n",
      "Epoch 32/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6905 - acc: 0.5448 - val_loss: 0.6892 - val_acc: 0.6111\n",
      "Epoch 33/128\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 0.6862 - acc: 0.6198 - val_loss: 0.6886 - val_acc: 0.6250\n",
      "Epoch 34/128\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.6898 - acc: 0.5605 - val_loss: 0.6896 - val_acc: 0.6389\n",
      "Epoch 35/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6857 - acc: 0.5836 - val_loss: 0.6909 - val_acc: 0.5833\n",
      "Epoch 36/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6899 - acc: 0.5605 - val_loss: 0.6897 - val_acc: 0.6111\n",
      "Epoch 37/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6865 - acc: 0.6130 - val_loss: 0.6876 - val_acc: 0.6458\n",
      "Epoch 38/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6874 - acc: 0.5743 - val_loss: 0.6891 - val_acc: 0.6389\n",
      "Epoch 39/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6886 - acc: 0.6026 - val_loss: 0.6923 - val_acc: 0.5556\n",
      "Epoch 40/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6894 - acc: 0.5833 - val_loss: 0.6896 - val_acc: 0.6111\n",
      "Epoch 41/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6882 - acc: 0.5907 - val_loss: 0.6889 - val_acc: 0.6250\n",
      "Epoch 42/128\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 0.6896 - acc: 0.5836 - val_loss: 0.6874 - val_acc: 0.6389\n",
      "Epoch 43/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6876 - acc: 0.5792 - val_loss: 0.6927 - val_acc: 0.5556\n",
      "Epoch 44/128\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.6875 - acc: 0.5521 - val_loss: 0.6886 - val_acc: 0.6389\n",
      "Epoch 45/128\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 0.6854 - acc: 0.6146 - val_loss: 0.6912 - val_acc: 0.5833\n",
      "Epoch 46/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6875 - acc: 0.5824 - val_loss: 0.6859 - val_acc: 0.6667\n",
      "Epoch 47/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6882 - acc: 0.5605 - val_loss: 0.6869 - val_acc: 0.6667\n",
      "Epoch 48/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6869 - acc: 0.6042 - val_loss: 0.6923 - val_acc: 0.5556\n",
      "Epoch 49/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6897 - acc: 0.5593 - val_loss: 0.6895 - val_acc: 0.6042\n",
      "Epoch 50/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6875 - acc: 0.5992 - val_loss: 0.6849 - val_acc: 0.6944\n",
      "Epoch 51/128\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6905 - acc: 0.5677 - val_loss: 0.6924 - val_acc: 0.5556\n",
      "Epoch 52/128\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.6902 - acc: 0.5762 - val_loss: 0.6895 - val_acc: 0.6111\n",
      "Epoch 53/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6872 - acc: 0.5888 - val_loss: 0.6872 - val_acc: 0.6458\n",
      "Epoch 54/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6877 - acc: 0.5836 - val_loss: 0.6903 - val_acc: 0.6111\n",
      "Epoch 55/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6918 - acc: 0.5469 - val_loss: 0.6859 - val_acc: 0.6389\n",
      "Epoch 56/128\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.6867 - acc: 0.6223 - val_loss: 0.6933 - val_acc: 0.5556\n",
      "Epoch 57/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6868 - acc: 0.5990 - val_loss: 0.6905 - val_acc: 0.5833\n",
      "Epoch 58/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6915 - acc: 0.5541 - val_loss: 0.6874 - val_acc: 0.6389\n",
      "Epoch 59/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6892 - acc: 0.5635 - val_loss: 0.6893 - val_acc: 0.6111\n",
      "Epoch 60/128\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.6852 - acc: 0.6354 - val_loss: 0.6877 - val_acc: 0.6389\n",
      "Epoch 61/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6859 - acc: 0.5965 - val_loss: 0.6890 - val_acc: 0.6042\n",
      "Epoch 62/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 86ms/step - loss: 0.6892 - acc: 0.5605 - val_loss: 0.6901 - val_acc: 0.6111\n",
      "Epoch 63/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6870 - acc: 0.6034 - val_loss: 0.6834 - val_acc: 0.6944\n",
      "Epoch 64/128\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.6894 - acc: 0.5938 - val_loss: 0.6925 - val_acc: 0.5556\n",
      "Epoch 65/128\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.6898 - acc: 0.5553 - val_loss: 0.6862 - val_acc: 0.6458\n",
      "Epoch 66/128\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.6878 - acc: 0.5877 - val_loss: 0.6949 - val_acc: 0.5278\n",
      "Epoch 67/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6884 - acc: 0.5680 - val_loss: 0.6890 - val_acc: 0.6111\n",
      "Epoch 68/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6866 - acc: 0.5784 - val_loss: 0.6852 - val_acc: 0.6667\n",
      "Epoch 69/128\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.6871 - acc: 0.5970 - val_loss: 0.6879 - val_acc: 0.6250\n",
      "Epoch 70/128\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6888 - acc: 0.5990 - val_loss: 0.6897 - val_acc: 0.6111\n",
      "Epoch 71/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6870 - acc: 0.5951 - val_loss: 0.6838 - val_acc: 0.6667\n",
      "Epoch 72/128\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6878 - acc: 0.5844 - val_loss: 0.6929 - val_acc: 0.5556\n",
      "Epoch 73/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6900 - acc: 0.5449 - val_loss: 0.6877 - val_acc: 0.6250\n",
      "Epoch 74/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6850 - acc: 0.6212 - val_loss: 0.6849 - val_acc: 0.6944\n",
      "Epoch 75/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6900 - acc: 0.5573 - val_loss: 0.6929 - val_acc: 0.5278\n",
      "Epoch 76/128\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.6907 - acc: 0.5575 - val_loss: 0.6884 - val_acc: 0.6111\n",
      "Epoch 77/128\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6867 - acc: 0.6201 - val_loss: 0.6891 - val_acc: 0.6042\n",
      "Epoch 78/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6870 - acc: 0.5806 - val_loss: 0.6896 - val_acc: 0.5833\n",
      "Epoch 79/128\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.6919 - acc: 0.5710 - val_loss: 0.6824 - val_acc: 0.7222\n",
      "Epoch 80/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6875 - acc: 0.5918 - val_loss: 0.6918 - val_acc: 0.5556\n",
      "Epoch 81/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6893 - acc: 0.5814 - val_loss: 0.6866 - val_acc: 0.6458\n",
      "Epoch 82/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6852 - acc: 0.6063 - val_loss: 0.6879 - val_acc: 0.6111\n",
      "Epoch 83/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6862 - acc: 0.5896 - val_loss: 0.6923 - val_acc: 0.5556\n",
      "Epoch 84/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6903 - acc: 0.5345 - val_loss: 0.6864 - val_acc: 0.6389\n",
      "Epoch 85/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6867 - acc: 0.5729 - val_loss: 0.6905 - val_acc: 0.5833\n",
      "Epoch 86/128\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6872 - acc: 0.5701 - val_loss: 0.6873 - val_acc: 0.6389\n",
      "Epoch 87/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6832 - acc: 0.6250 - val_loss: 0.6868 - val_acc: 0.6111\n",
      "Epoch 88/128\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6883 - acc: 0.5907 - val_loss: 0.6869 - val_acc: 0.6389\n",
      "Epoch 89/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6902 - acc: 0.5586 - val_loss: 0.6904 - val_acc: 0.5833\n",
      "Epoch 90/128\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6847 - acc: 0.6022 - val_loss: 0.6866 - val_acc: 0.6389\n",
      "Epoch 91/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6865 - acc: 0.5739 - val_loss: 0.6824 - val_acc: 0.6944\n",
      "Epoch 92/128\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6875 - acc: 0.5888 - val_loss: 0.6916 - val_acc: 0.5556\n",
      "Epoch 93/128\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.6868 - acc: 0.5572 - val_loss: 0.6905 - val_acc: 0.5833\n",
      "Epoch 94/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6862 - acc: 0.5792 - val_loss: 0.6866 - val_acc: 0.6389\n",
      "Epoch 95/128\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.6864 - acc: 0.6138 - val_loss: 0.6893 - val_acc: 0.5833\n",
      "Epoch 96/128\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6913 - acc: 0.5521 - val_loss: 0.6840 - val_acc: 0.6667\n",
      "Epoch 97/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6837 - acc: 0.6297 - val_loss: 0.6869 - val_acc: 0.6250\n",
      "Epoch 98/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6874 - acc: 0.5781 - val_loss: 0.6884 - val_acc: 0.6111\n",
      "Epoch 99/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6885 - acc: 0.5511 - val_loss: 0.6866 - val_acc: 0.6389\n",
      "Epoch 100/128\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6836 - acc: 0.6410 - val_loss: 0.6892 - val_acc: 0.5833\n",
      "Epoch 101/128\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 0.6921 - acc: 0.5521 - val_loss: 0.6854 - val_acc: 0.6458\n",
      "Epoch 102/128\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 0.6840 - acc: 0.6022 - val_loss: 0.6928 - val_acc: 0.5556\n",
      "Epoch 103/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6881 - acc: 0.5522 - val_loss: 0.6841 - val_acc: 0.6389\n",
      "Epoch 104/128\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.6861 - acc: 0.5990 - val_loss: 0.6888 - val_acc: 0.6111\n",
      "Epoch 105/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6871 - acc: 0.5899 - val_loss: 0.6881 - val_acc: 0.6042\n",
      "Epoch 106/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6866 - acc: 0.5866 - val_loss: 0.6888 - val_acc: 0.6111\n",
      "Epoch 107/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6853 - acc: 0.5989 - val_loss: 0.6865 - val_acc: 0.6111\n",
      "Epoch 108/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6906 - acc: 0.5605 - val_loss: 0.6863 - val_acc: 0.6389\n",
      "Epoch 109/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6845 - acc: 0.6201 - val_loss: 0.6883 - val_acc: 0.6042\n",
      "Epoch 110/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6907 - acc: 0.5627 - val_loss: 0.6918 - val_acc: 0.5556\n",
      "Epoch 111/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6907 - acc: 0.5469 - val_loss: 0.6873 - val_acc: 0.6389\n",
      "Epoch 112/128\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.6798 - acc: 0.6182 - val_loss: 0.6817 - val_acc: 0.6667\n",
      "Epoch 113/128\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.6904 - acc: 0.5531 - val_loss: 0.6897 - val_acc: 0.5833\n",
      "Epoch 114/128\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6848 - acc: 0.5995 - val_loss: 0.6793 - val_acc: 0.7222\n",
      "Epoch 115/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6868 - acc: 0.5938 - val_loss: 0.6926 - val_acc: 0.5278\n",
      "Epoch 116/128\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 0.6868 - acc: 0.6078 - val_loss: 0.6867 - val_acc: 0.6389\n",
      "Epoch 117/128\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.6874 - acc: 0.5784 - val_loss: 0.6883 - val_acc: 0.6042\n",
      "Epoch 118/128\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6894 - acc: 0.5814 - val_loss: 0.6779 - val_acc: 0.7222\n",
      "Epoch 119/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6884 - acc: 0.5698 - val_loss: 0.6942 - val_acc: 0.5278\n",
      "Epoch 120/128\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6854 - acc: 0.5940 - val_loss: 0.6877 - val_acc: 0.6111\n",
      "Epoch 121/128\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6890 - acc: 0.5594 - val_loss: 0.6884 - val_acc: 0.6042\n",
      "Epoch 122/128\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.6880 - acc: 0.5990 - val_loss: 0.6871 - val_acc: 0.6111\n",
      "Epoch 123/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 93ms/step - loss: 0.6854 - acc: 0.6111 - val_loss: 0.6869 - val_acc: 0.6111\n",
      "Epoch 124/128\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.6870 - acc: 0.5729 - val_loss: 0.6853 - val_acc: 0.6389\n",
      "Epoch 125/128\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 0.6875 - acc: 0.5639 - val_loss: 0.6848 - val_acc: 0.6458\n",
      "Epoch 126/128\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 0.6837 - acc: 0.6034 - val_loss: 0.6890 - val_acc: 0.5833\n",
      "Epoch 127/128\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.6894 - acc: 0.5646 - val_loss: 0.6958 - val_acc: 0.5278\n",
      "Epoch 128/128\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.6875 - acc: 0.5844 - val_loss: 0.6788 - val_acc: 0.6944\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=ntrain // batch_size,\n",
    "                              epochs=128,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=nval // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
